{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4e5949-7167-456b-9b16-b392bd59f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "# import concurrent.futures\n",
    "\n",
    "# # Function to scrape Real Python\n",
    "# def scrape_realpython():\n",
    "#     url = \"https://realpython.com\"\n",
    "#     search_url = f\"{url}/search?q=concurrency+in+python\"\n",
    "#     response = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     # Find the list of articles\n",
    "#     articles = soup.find_all('article')\n",
    "#     data = []\n",
    "\n",
    "#     for article in articles:\n",
    "#         title = article.find('h2').text.strip() if article.find('h2') else \"No Title\"\n",
    "#         link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "#         full_url = f\"{url}{link}\"\n",
    "#         published_date = article.find('time').text.strip() if article.find('time') else \"N/A\"\n",
    "#         level = \"Intermediate\"  # Assuming most articles have this level, adjust based on content.\n",
    "#         summary = article.find('p').text.strip() if article.find('p') else \"No summary available\"\n",
    "\n",
    "#         data.append({\n",
    "#             'Website': 'Real Python',\n",
    "#             'Title': title,\n",
    "#             'URL': full_url,\n",
    "#             'Published Date': published_date,\n",
    "#             'Level': level,\n",
    "#             'Summary': summary[:300]  # Summarize first 200-300 characters\n",
    "#         })\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # Function to scrape Towards Data Science\n",
    "# def scrape_towardsdatascience():\n",
    "#     url = \"https://towardsdatascience.com\"\n",
    "#     search_url = f\"{url}/search?q=concurrency+in+python\"\n",
    "#     response = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     # Find the list of articles\n",
    "#     articles = soup.find_all('article')\n",
    "#     data = []\n",
    "\n",
    "#     for article in articles:\n",
    "#         title = article.find('h3').text.strip() if article.find('h3') else \"No Title\"\n",
    "#         link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "#         full_url = f\"{url}{link}\"\n",
    "#         published_date = article.find('time').text.strip() if article.find('time') else \"N/A\"\n",
    "#         level = \"Intermediate\"  # Adjust based on actual content\n",
    "#         summary = article.find('p').text.strip() if article.find('p') else \"No summary available\"\n",
    "\n",
    "#         data.append({\n",
    "#             'Website': 'Towards Data Science',\n",
    "#             'Title': title,\n",
    "#             'URL': full_url,\n",
    "#             'Published Date': published_date,\n",
    "#             'Level': level,\n",
    "#             'Summary': summary[:300]  # Summarize first 200-300 characters\n",
    "#         })\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # Function to scrape Geeks for Geeks\n",
    "# def scrape_geeksforgeeks():\n",
    "#     url = \"https://www.geeksforgeeks.org\"\n",
    "#     search_url = f\"{url}/?s=Concurrency+in+Python\"\n",
    "#     response = requests.get(search_url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     # Find the list of articles\n",
    "#     articles = soup.find_all('article')\n",
    "#     data = []\n",
    "\n",
    "#     for article in articles:\n",
    "#         title = article.find('h2').text.strip() if article.find('h2') else \"No Title\"\n",
    "#         link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "#         full_url = f\"{url}{link}\"\n",
    "#         published_date = article.find('time').text.strip() if article.find('time') else \"N/A\"\n",
    "#         level = \"Intermediate\"  # Adjust based on actual content\n",
    "#         summary = article.find('p').text.strip() if article.find('p') else \"No summary available\"\n",
    "\n",
    "#         data.append({\n",
    "#             'Website': 'Geeks for Geeks',\n",
    "#             'Title': title,\n",
    "#             'URL': full_url,\n",
    "#             'Published Date': published_date,\n",
    "#             'Level': level,\n",
    "#             'Summary': summary[:300]  # Summarize first 200-300 characters\n",
    "#         })\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # Function to save scraped data to CSV\n",
    "# def save_to_csv(data):\n",
    "#     with open('scraped_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.DictWriter(file, fieldnames=['Website', 'Title', 'URL', 'Published Date', 'Level', 'Summary'])\n",
    "#         writer.writeheader()\n",
    "#         for row in data:\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# # Function to scrape all websites concurrently\n",
    "# def scrape_all_websites():\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         results = executor.map(lambda func: func(), [scrape_realpython, scrape_towardsdatascience, scrape_geeksforgeeks])\n",
    "    \n",
    "#     # Flatten the results\n",
    "#     all_data = [item for sublist in results for item in sublist]\n",
    "    \n",
    "#     # Save the scraped data to CSV\n",
    "#     save_to_csv(all_data)\n",
    "#     print(\"Data scraped and saved to scraped_data.csv\")\n",
    "\n",
    "# # Run the scraping process\n",
    "# scrape_all_websites()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e51a9771-8b00-46fc-8f80-b312b42a2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Real Python...\n",
      "Scraping Towards Data Science...\n",
      "Scraping Geeks for Geeks...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'scraped_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     save_to_csv(all_data)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Run the scraper\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mscrape_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 107\u001b[0m, in \u001b[0;36mscrape_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping Geeks for Geeks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m all_data\u001b[38;5;241m.\u001b[39mextend(scrape_geeksforgeeks())\n\u001b[1;32m--> 107\u001b[0m \u001b[43msave_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 90\u001b[0m, in \u001b[0;36msave_to_csv\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_csv\u001b[39m(data, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     91\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebsite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPublished Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     92\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'scraped_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "}\n",
    "url=\"https://realpython.com\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "# Function to scrape Real Python\n",
    "def scrape_realpython():\n",
    "    base_url = \"https://realpython.com\"\n",
    "    search_url = f\"{base_url}/search?q=concurrency+in+python\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = soup.select('div.card.border-0')  # Select article cards\n",
    "    data = []\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h2').get_text(strip=True) if article.find('h2') else \"No Title\"\n",
    "        link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "        published_date = article.find('time').get_text(strip=True) if article.find('time') else \"N/A\"\n",
    "        summary = article.find('p').get_text(strip=True) if article.find('p') else \"No summary available\"\n",
    "\n",
    "        data.append({\n",
    "            'Website': 'Real Python',\n",
    "            'Title': title,\n",
    "            'URL': base_url + link,\n",
    "            'Published Date': published_date,\n",
    "            'Level': 'Intermediate',  # Update logic if required\n",
    "            'Summary': summary[:300]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Function to scrape Towards Data Science\n",
    "def scrape_towardsdatascience():\n",
    "    base_url = \"https://towardsdatascience.com\"\n",
    "    search_url = f\"{base_url}/search?q=concurrency+in+python\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = soup.select('div.postArticle-content')  # Select articles\n",
    "    data = []\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h3').get_text(strip=True) if article.find('h3') else \"No Title\"\n",
    "        link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "        summary = article.find('p').get_text(strip=True) if article.find('p') else \"No summary available\"\n",
    "\n",
    "        data.append({\n",
    "            'Website': 'Towards Data Science',\n",
    "            'Title': title,\n",
    "            'URL': link.split('?')[0],  # Clean the link\n",
    "            'Published Date': \"N/A\",  # Publication date not always available\n",
    "            'Level': 'Intermediate',  # Update logic if required\n",
    "            'Summary': summary[:300]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Function to scrape Geeks for Geeks\n",
    "def scrape_geeksforgeeks():\n",
    "    base_url = \"https://www.geeksforgeeks.org\"\n",
    "    search_url = f\"{base_url}/?s=Concurrency+in+Python\"\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = soup.select('div.head')  # Select articles\n",
    "    data = []\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('a').get_text(strip=True) if article.find('a') else \"No Title\"\n",
    "        link = article.find('a')['href'] if article.find('a') else \"No URL\"\n",
    "        summary = article.find_next('p').get_text(strip=True) if article.find_next('p') else \"No summary available\"\n",
    "\n",
    "        data.append({\n",
    "            'Website': 'Geeks for Geeks',\n",
    "            'Title': title,\n",
    "            'URL': link,\n",
    "            'Published Date': \"N/A\",  # Geeks for Geeks often lacks dates\n",
    "            'Level': 'Beginner',  # Update logic if required\n",
    "            'Summary': summary[:300]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# Function to save data to a CSV file\n",
    "def save_to_csv(data, filename='scraped_data.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['Website', 'Title', 'URL', 'Published Date', 'Level', 'Summary'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function to scrape all websites\n",
    "def scrape_all():\n",
    "    all_data = []\n",
    "    print(\"Scraping Real Python...\")\n",
    "    all_data.extend(scrape_realpython())\n",
    "    print(\"Scraping Towards Data Science...\")\n",
    "    all_data.extend(scrape_towardsdatascience())\n",
    "    print(\"Scraping Geeks for Geeks...\")\n",
    "    all_data.extend(scrape_geeksforgeeks())\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289e820-503f-402d-9aa5-88b586020a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
